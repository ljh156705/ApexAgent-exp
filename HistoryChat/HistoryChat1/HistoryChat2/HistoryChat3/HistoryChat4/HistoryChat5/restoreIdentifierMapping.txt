{'b6f0638d0bc2481bb64b7fbbe399acb7__import torch\nimport torch.nn as nn\n\nclass LSTMModel(nn.Module):\n    """\n    A simple LSTM model for sequence prediction.\n    """\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        """\n        Initializes the LSTM model.\n        Args:\n            input_size (int): The number of expected features in the input x.\n            hidden_size (int): The number of features in the hidden state h.\n            num_layers (int): Number of recurrent layers.\n            output_size (int): The number of features in the output.\n        """\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Define the LSTM layer\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        \n        # Define the output layer\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        """\n        Defines the forward pass of the model.\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, seq_length, input_size).\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, output_size).\n        """\n        # Initialize hidden and cell states with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        \n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don\'t, we\'ll backprop all the way to the start of the dataset\n        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\nif __name__ == \'__main__\':\n    # Example of how to instantiate the model\n    # These are placeholder values and should be adjusted for a real task\n    input_dim = 10\n    hidden_dim = 20\n    layer_dim = 1\n    output_dim = 1\n\n    model = LSTMModel(input_size=input_dim, hidden_size=hidden_dim, num_layers=layer_dim, output_size=output_dim)\n    print("LSTM Model created successfully!")\n    print(model)\n': ['lstm_model.py', '<Non-compressible placeholder 1>'], '2ef8432d4ff1425983a19a1ded172bea__import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom lstm_model import LSTMModel\n\n# --- Hyperparameters ---\n# These should be adjusted based on the specific dataset and task\nINPUT_SIZE = 10       # Dimension of the input features\nHIDDEN_SIZE = 64      # Number of features in the hidden state\nNUM_LAYERS = 2        # Number of LSTM layers\nOUTPUT_SIZE = 1       # Dimension of the output\nLEARNING_RATE = 0.001\nNUM_EPOCHS = 25\nBATCH_SIZE = 32\nMODEL_SAVE_PATH = "lstm_trained_model.pth"\n\ndef train_model():\n    """\n    This function simulates the training process of the LSTM model.\n    In a real-world scenario, you would load your data here.\n    """\n    print("Starting model training...")\n\n    # 1. Instantiate the model\n    model = LSTMModel(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=OUTPUT_SIZE)\n    print("Model instantiated:")\n    print(model)\n\n    # 2. Create dummy data for demonstration\n    # In a real application, you would use a DataLoader to load your actual dataset.\n    # Input shape: (batch_size, sequence_length, input_size)\n    # Target shape: (batch_size, output_size)\n    dummy_input = torch.randn(BATCH_SIZE, 5, INPUT_SIZE) # 5 is the sequence length\n    dummy_target = torch.randn(BATCH_SIZE, OUTPUT_SIZE)\n    print(f"\\\\nCreated dummy data with input shape: {dummy_input.shape}")\n\n    # 3. Define Loss and Optimizer\n    criterion = nn.MSELoss() # Mean Squared Error loss is common for regression tasks\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    print(f"Using loss function: {type(criterion).__name__}")\n    print(f"Using optimizer: {type(optimizer).__name__}")\n\n    # 4. Training Loop\n    model.train() # Set the model to training mode\n    for epoch in range(NUM_EPOCHS):\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(dummy_input)\n        loss = criterion(outputs, dummy_target)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        if (epoch+1) % 5 == 0:\n            print(f\'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}\')\n\n    print("\\\\nTraining finished.")\n\n    # 5. Save the trained model\n    try:\n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        print(f"Model saved successfully to {MODEL_SAVE_PATH}")\n    except Exception as e:\n        print(f"Error saving model: {e}")\n\n\nif __name__ == \'__main__\':\n    train_model()\n': ['train_model.py', '<Non-compressible placeholder 2>'], 'ac68a0a2ba404da1b679a4969ca016c8__import torch\nfrom lstm_model import LSTMModel\n\n# --- Parameters ---\n# These MUST match the parameters used during training in train_model.py\nINPUT_SIZE = 10\nHIDDEN_SIZE = 64\nNUM_LAYERS = 2\nOUTPUT_SIZE = 1\nMODEL_PATH = "lstm_trained_model.pth"\n\ndef predict():\n    """\n    Loads the trained LSTM model and makes a prediction on new data.\n    """\n    print("Starting prediction process...")\n\n    # 1. Instantiate the model with the same architecture\n    model = LSTMModel(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=OUTPUT_SIZE)\n    print("Model instantiated.")\n\n    # 2. Load the trained model weights\n    try:\n        model.load_state_dict(torch.load(MODEL_PATH))\n        print(f"Model weights loaded successfully from {MODEL_PATH}")\n    except FileNotFoundError:\n        print(f"Error: Model file not found at {MODEL_PATH}.")\n        print("Please run train_model.py first to train and save the model.")\n        return\n    except Exception as e:\n        print(f"An error occurred while loading the model: {e}")\n        return\n\n    # 3. Set the model to evaluation mode\n    # This is important for layers like Dropout and BatchNorm, which behave differently during training and evaluation\n    model.eval()\n    print("Model set to evaluation mode.")\n\n    # 4. Create dummy input data for prediction\n    # In a real application, this would be your new, unseen data.\n    # Input shape: (batch_size, sequence_length, input_size)\n    # Here, batch_size is 1 as we are predicting for a single new sequence.\n    dummy_input = torch.randn(1, 5, INPUT_SIZE) # 5 is the sequence length\n    print(f"\\\\nCreated dummy input data with shape: {dummy_input.shape}")\n\n    # 5. Make a prediction\n    print("Making prediction...")\n    with torch.no_grad(): # Deactivates autograd engine, reduces memory usage and speeds up computations\n        prediction = model(dummy_input)\n    \n    print(f"\\\\nPrediction successful.")\n    print(f"Predicted output: {prediction.item():.4f}")\n\nif __name__ == \'__main__\':\n    predict()\n': ['predict.py', '<Non-compressible placeholder 3>'], '921a6e7ba99e4ee28d4a2b8606fe8016__import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom lstm_model import LSTMModel\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# --- Hyperparameters ---\nINPUT_SIZE = 10\nHIDDEN_SIZE = 64\nNUM_LAYERS = 2\nOUTPUT_SIZE = 1\nLEARNING_RATE = 0.001\nNUM_EPOCHS = 50 # Increased epochs for better training\nBATCH_SIZE = 32\nMODEL_SAVE_PATH = "lstm_trained_model_v2.pth"\nVISUALIZATION_PATH = "performance_visualization.png"\n\ndef train_and_evaluate_model():\n    """\n    Trains the LSTM model, evaluates it on a test set with multiple metrics,\n    and generates a visualization.\n    """\n    print("Starting model training and evaluation process...")\n\n    # 1. Instantiate the model\n    model = LSTMModel(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=OUTPUT_SIZE)\n    \n    # 2. Create dummy datasets for training and testing\n    # In a real application, you would use a DataLoader with your actual datasets.\n    train_input = torch.randn(BATCH_SIZE * 5, 5, INPUT_SIZE) # 5 batches for training\n    train_target = torch.randn(BATCH_SIZE * 5, OUTPUT_SIZE)\n    \n    test_input = torch.randn(BATCH_SIZE * 2, 5, INPUT_SIZE) # 2 batches for testing\n    test_target = torch.randn(BATCH_SIZE * 2, OUTPUT_SIZE)\n    print(f"Created dummy data: {train_input.shape[0]} training samples, {test_input.shape[0]} testing samples.")\n\n    # 3. Define Loss and Optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    # 4. Training Loop\n    print("\\\\n--- Training ---")\n    model.train()\n    for epoch in range(NUM_EPOCHS):\n        optimizer.zero_grad()\n        outputs = model(train_input)\n        loss = criterion(outputs, train_target)\n        loss.backward()\n        optimizer.step()\n        if (epoch+1) % 10 == 0:\n            print(f\'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}\')\n    print("Training finished.")\n\n    # 5. Save the trained model\n    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n    print(f"Model saved to {MODEL_SAVE_PATH}")\n\n    # 6. Evaluation\n    print("\\\\n--- Evaluation ---")\n    model.eval()\n    with torch.no_grad():\n        predictions = model(test_input)\n    \n    # Convert to numpy for metric calculation\n    actuals = test_target.numpy()\n    preds = predictions.numpy()\n\n    # Calculate metrics\n    mse = criterion(predictions, test_target).item()\n    mae = mean_absolute_error(actuals, preds)\n    r2 = r2_score(actuals, preds)\n\n    print(f"Evaluation Metrics on Test Set:")\n    print(f"  - Mean Squared Error (MSE): {mse:.4f}")\n    print(f"  - Mean Absolute Error (MAE): {mae:.4f}")\n    print(f"  - R-squared (R2 Score):     {r2:.4f}")\n\n    # 7. Visualization\n    print("\\\\n--- Visualization ---")\n    plt.figure(figsize=(8, 6))\n    plt.scatter(actuals, preds, alpha=0.6, edgecolors=\'k\')\n    plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], \'r--\', lw=2)\n    plt.title(\'Actual vs. Predicted Values\')\n    plt.xlabel(\'Actual Values\')\n    plt.ylabel(\'Predicted Values\')\n    plt.grid(True)\n    try:\n        plt.savefig(VISUALIZATION_PATH)\n        print(f"Visualization saved successfully to {VISUALIZATION_PATH}")\n    except Exception as e:\n        print(f"Error saving visualization: {e}")\n\nif __name__ == \'__main__\':\n    train_and_evaluate_model()\n': ['train_and_evaluate_model.py', '<Non-compressible placeholder 4>']}